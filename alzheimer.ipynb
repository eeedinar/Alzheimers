{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from enum import Enum\n",
    "from py4xs.detector_config import create_det_from_attrs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from essential_func import *\n",
    "\n",
    "os.chdir('/Users/bashit.a/Documents/Alzheimer/Dec-2020')\n",
    "\n",
    "class h5exp():\n",
    "    \"\"\" empty h5 file for exchanging exp_setup/qgrid\n",
    "    \"\"\"\n",
    "    def __init__(self, fn, exp_setup=None):\n",
    "        self.fn = fn\n",
    "        if exp_setup==None:     # assume the h5 file will provide the detector config\n",
    "            self.qgrid = self.read_detectors()\n",
    "        else:\n",
    "            self.detectors, self.qgrid = exp_setup\n",
    "            self.save_detectors()\n",
    "        \n",
    "    def save_detectors(self):\n",
    "        self.fh5 = h5py.File(self.fn, \"w\")   # new file\n",
    "        dets_attr = [det.pack_dict() for det in self.detectors]\n",
    "        self.fh5.attrs['detectors'] = json.dumps(dets_attr)\n",
    "        self.fh5.attrs['qgrid'] = list(self.qgrid)\n",
    "        self.fh5.flush()\n",
    "        self.fh5.close()\n",
    "    \n",
    "    def read_detectors(self):\n",
    "        self.fh5 = h5py.File(self.fn, \"r\")   # file must exist\n",
    "        dets_attr = self.fh5.attrs['detectors']\n",
    "        qgrid = self.fh5.attrs['qgrid']\n",
    "        self.detectors = [create_det_from_attrs(attrs) for attrs in json.loads(dets_attr)]  \n",
    "        self.fh5.close()\n",
    "        return np.asarray(qgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_WAXS2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d4c48552c8454699f002d893f809c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_para.Q Min SAXS = 0.00016180539977154286 \n",
      "exp_para.Q Max SAXS = 0.37565671178795346\n",
      "SAXS Mask Shape (981, 1043), 1.0 (white) is masked means ignore this pixel \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565d69c6006f4a89a83133f2212b3560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_para.Q Min WAXS = 0.0008471915121232293       \n",
      "exp_para.Q Max WAXS = 3.369008660772031\n",
      "WAXS Mask Shape (1043, 981), 1.0 (white) is masked means ignore this pixel \n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "de = h5exp(\"exp.h5\")\n",
    "qgrid2 = np.hstack([np.arange(0.005, 0.0499, 0.001), np.arange(0.05, 0.099, 0.002), np.arange(0.1, 3.2, 0.005)])\n",
    "print(de.detectors[1].extension)\n",
    "np.min(de.detectors[1].exp_para.Q), np.mean(de.detectors[1].exp_para.Q), np.max(de.detectors[1].exp_para.Q)\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1,4, num='SAXS', figsize=(16,5))\n",
    "\n",
    "SAXS_Q = de.detectors[0].exp_para.Q\n",
    "im = axs[0].imshow(SAXS_Q, cmap=\"jet\")\n",
    "show_colorbar(im,f,axs[0])\n",
    "axs[0].set_title('saxs.exp_para.Q')\n",
    "print(f'exp_para.Q Min SAXS = {np.min(SAXS_Q)} \\nexp_para.Q Max SAXS = {np.max(SAXS_Q)}')\n",
    "\n",
    "SAXS_MASK = de.detectors[0].exp_para.mask.map\n",
    "print('SAXS Mask Shape {}, 1.0 (white) is masked means ignore this pixel '.format(SAXS_MASK.shape))\n",
    "axs[1].imshow(SAXS_MASK,cmap='gray')\n",
    "axs[1].set_title('saxs.exp_para.mask.map')\n",
    "\n",
    "im = axs[2].imshow(SAXS_Q*~SAXS_MASK, cmap='jet')\n",
    "show_colorbar(im,f,axs[2])\n",
    "axs[2].set_title('SAXS_Q*~SAXS_MASK')\n",
    "\n",
    "\n",
    "SAXS_cor_factor = de.detectors[0].exp_para.FSA*de.detectors[0].exp_para.FPol\n",
    "im = axs[3].imshow(SAXS_cor_factor, cmap='jet')\n",
    "show_colorbar(im,f,axs[3])\n",
    "axs[3].set_title('SAXS_cor_factor')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "f, axs = plt.subplots(1,4, num='WAXS', figsize=(16,5))\n",
    "WAXS_Q = de.detectors[1].exp_para.Q\n",
    "im = axs[0].imshow(WAXS_Q, cmap=\"jet\")\n",
    "show_colorbar(im,f,axs[0])\n",
    "axs[0].set_title('waxs.exp_para.Q')\n",
    "print(f'exp_para.Q Min WAXS = {np.min(WAXS_Q)} \\\n",
    "      \\nexp_para.Q Max WAXS = {np.max(WAXS_Q)}')\n",
    "\n",
    "WAXS_MASK = de.detectors[1].exp_para.mask.map\n",
    "print('WAXS Mask Shape {}, 1.0 (white) is masked means ignore this pixel '.format(WAXS_MASK.shape))\n",
    "axs[1].imshow(WAXS_MASK,cmap='gray')\n",
    "axs[1].set_title('waxs.exp_para.mask.map')\n",
    "\n",
    "im = axs[2].imshow(WAXS_Q*~WAXS_MASK, cmap='jet')\n",
    "show_colorbar(im,f,axs[2])\n",
    "axs[2].set_title('WAXS_Q*~WAXS_MASK')\n",
    "\n",
    "WAXS_cor_factor = de.detectors[1].exp_para.FSA*de.detectors[1].exp_para.FPol\n",
    "im = axs[3].imshow(WAXS_cor_factor, cmap='jet')\n",
    "show_colorbar(im,f,axs[3])\n",
    "axs[3].set_title('WAXS_cor_factor')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28605dcd24524d5aa8007e1a9dc4ba69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x156f639a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "WAXS_Q_mask = cv2.rectangle(WAXS_Q, (350,360), (550,660), -1, -1)\n",
    "plt.imshow(WAXS_Q_mask, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1043, 981)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.detectors[1].ImageHeight, de.detectors[1].ImageWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 starts at  0.005 ends at  0.048999999999999995  0.002 starts  0.05 ends  0.09800000000000005  0.005 starts at  0.1 ends at  3.195000000000003\n",
      "-1.0 3.369008660772031\n"
     ]
    }
   ],
   "source": [
    "print('0.001 starts at ', qgrid2[0],  'ends at ', qgrid2[44], ' 0.002 starts ', qgrid2[45], 'ends ', qgrid2[69],' 0.005 starts at ', qgrid2[70], 'ends at ', qgrid2[689])\n",
    "print(WAXS_Q.min(), WAXS_Q.max())\n",
    "\n",
    "file = '2048_B8_masked.h5'\n",
    "with h5py.File(file,'r') as hdf:\n",
    "    dset_waxs = np.array(hdf.get(f'{h5_top_group(file)}/primary/data/pilW2_image'))         # waxs data read from h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 2223\n",
    "dataD = dset_waxs[frame]\n",
    "min_norm_scale=0.002\n",
    "interpolate = True\n",
    "dataD = np.ones_like(WAXS_cor_factor)\n",
    "dataD = WAXS_Q_mask\n",
    "maskMap = WAXS_MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def azimuthal_avg(dataD, qgrid, maskMap, cor_factor, \n",
    "            adjust_edges=True, interpolate=True, min_norm_scale=0.002):\n",
    "\n",
    "    dd = dataD/WAXS_cor_factor                                              # divide all data by correction factor\n",
    "\n",
    "    # Pilatus might use negative values to mark dead pixels\n",
    "    idx = (dataD>=0)                                                        # idx of values >=0\n",
    "    idx &= ~(maskMap)\n",
    "\n",
    "    qd = WAXS_Q[idx].flatten()\n",
    "    dd = np.asarray(dd[idx].flatten(), dtype=float)                      # 2D flattened to 1D other wise dd*dd might become negative\n",
    "\n",
    "    qgrid = qgrid2\n",
    "    adjust_edges = True\n",
    "\n",
    "    if adjust_edges:\n",
    "        # willing to throw out some data, but the edges strictly correspond to qgrid\n",
    "        dq  = qgrid[1:]-qgrid[:-1]                                          # difference between two consequtive q values\n",
    "        dq1 = np.hstack(([dq[0]], dq))                                      # len(dq1) == len(qgrid)\n",
    "\n",
    "        bins = [qgrid[0]-dq1[0]/2]                                          # one value/ lower limit value qgrid[0]=0.005, dq1[0] = 0.001; bins = qgrid[0]-dq1[0]/2 = 0.0045 \n",
    "        bidx = []\n",
    "        binw = []\n",
    "\n",
    "        # bins will be len(qgrid) + 1\n",
    "        # bidx, binw will be len(qgrid)\n",
    "        for i in range(len(qgrid)):\n",
    "            el = qgrid[i] - dq1[i]/2                                        # qgrid[0]=0.005; el = qgrid[0]-dq1[0]/2 = 0.0045\n",
    "            eu = qgrid[i] + dq1[i]/2                                        # qgrid[0]=0.005; eu = qgrid[0]+dq1[0]/2 = 0.0055\n",
    "            if i==0 or np.fabs(el-bins[-1])<dq1[i]/100:                     # i=0 first element or \n",
    "                bins += [eu]                                                # first bin bins = [0.0045, 0.0055]\n",
    "                bidx += [True]                                              # \n",
    "                binw += [dq1[i]]                                            # \n",
    "            else:\n",
    "                bins += [el, eu]                                            # \n",
    "                bidx += [False, True]                                       # \n",
    "                binw += [dq1[i-1], dq1[i]]                                  # \n",
    "                #print(qgrid[i], '\\n\\n', bins ,'\\n\\n', bidx, '\\n\\n', binw )\n",
    "                #print('.......')\n",
    "    else:\n",
    "        # keep all the data, but the histogrammed data will be less accurate \n",
    "        bins =  np.append([2*qgrid[0]-qgrid[1]], qgrid)\n",
    "        bins += np.append(qgrid , [2*qgrid[-1]-qgrid[-2]])\n",
    "        bins *= 0.5\n",
    "        bidx = np.ones(len(qgrid), dtype=bool)\n",
    "\n",
    "    norm = np.histogram(qd, bins=bins, weights=np.ones(len(qd)))[0][bidx]\n",
    "    qq   = np.histogram(qd, bins=bins, weights=qd)[0][bidx]\n",
    "    Iq   = np.histogram(qd, bins=bins, weights=dd)[0][bidx]\n",
    "    Iq2  = np.histogram(qd, bins=bins, weights=dd*dd)[0][bidx]\n",
    "\n",
    "    idx1 = (norm>min_norm_scale*np.arange(len(norm))**2)\n",
    "    qq[idx1]  /= norm[idx1]\n",
    "    Iq[idx1]  /= norm[idx1]   \n",
    "    Iq2[idx1] /= norm[idx1]\n",
    "    dI = np.sqrt(Iq2-Iq*Iq)\n",
    "    dI[idx1] /= np.sqrt(norm[idx1])\n",
    "    qq[~idx1] = np.nan\n",
    "    Iq[~idx1] = np.nan\n",
    "    dI[~idx1] = np.nan\n",
    "\n",
    "    if interpolate:\n",
    "        Iq = np.interp(qgrid, qq, Iq)\n",
    "    return Iq, dI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.90562316 2.90239235 2.89916191 ... 2.43355992 2.43590933 2.43826246]\n",
      "0.28369898835062113 3.137054977863803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cf59626cc74c96b4ee876e176926e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a76ac42fd2468c9530e8b4ce732b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56a2c213ce842f58877cbf47c678100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-310-da9c40d8c753>:56: RuntimeWarning: invalid value encountered in sqrt\n",
      "  dI = np.sqrt(Iq2-Iq*Iq)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f56dbba2de4234aedf5b6b28ea2f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x31ba59310>]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "bins = np.array(bins)\n",
    "bidx = np.array(bidx)\n",
    "# print(qgrid2)\n",
    "# print('........')\n",
    "# print(bins)\n",
    "# print('........')\n",
    "data = [[0,1,1],\n",
    "        [2,1,2]]\n",
    "weights = [[9,10,11],\n",
    "           [20,12,22]]\n",
    "\n",
    "ones = np.ones_like(data)\n",
    "hist, bin_edges = np.histogram(data, bins=[0,1,2,3], weights = ones)\n",
    "\n",
    "hist\n",
    "\n",
    "print(qd)\n",
    "print(qd.min(), qd.max())\n",
    "# print([np.round(float(i), 4) for i in bins])\n",
    "plt.scatter(qgrid2,qq)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(qgrid2,norm)\n",
    "\n",
    "f, axs = plt.subplots(1,2, num=f'frame = {frame}',figsize=(14,5))\n",
    "\n",
    "Iq_no_Mask, _ = azimuthal_avg(WAXS_Q, qgrid2, WAXS_MASK, WAXS_cor_factor)\n",
    "Iq_Masked, _ = azimuthal_avg(WAXS_Q_mask, qgrid2, WAXS_MASK, WAXS_cor_factor)\n",
    "\n",
    "axs[0].plot(qgrid2,Iq_no_Mask,label='without')\n",
    "axs[1].plot(qgrid2,Iq_Masked, label='masked')\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "\n",
    "len(np.where( (qd >=1.4925) & (qd <=1.4975) )[0])\n",
    "#print(norm)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(np.histogram(qd, bins=bins, weights=dd)[0][bidx])), np.histogram(qd, bins=bins, weights=dd)[0][bidx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low valid idx = 109, low valid Q = 0.295, high valid idx = 578 , high valid Q = 2.640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78071f8a1d61482496774d4f78f841a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd13094075d44c5bd212a17121599f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "idx_l, idx_u, valid_diff_values = valid_idx_search(qgrid2, Iq[np.newaxis,:])\n",
    "plt.plot(qgrid2[idx_l:idx_u],Iq[idx_l:idx_u])\n",
    "\n",
    "DD = dataD/WAXS_cor_factor\n",
    "f, ax = plt.subplots()\n",
    "im = plt.imshow(DD)\n",
    "show_colorbar(im,f,ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "def conv_Iq(self, qgrid, mask=None, cor_factor=1, \n",
    "            adjust_edges=True, interpolate=True, min_norm_scale=0.002):\n",
    "\n",
    "    dd = self.data.d/cor_factor                                             # divide all data by correction factor\n",
    "\n",
    "    # Pilatus might use negative values to mark dead pixels\n",
    "    idx = (self.data.d>=0)                                                  # idx of values >=0\n",
    "    if mask is not None:\n",
    "        idx &= ~(mask.map)\n",
    "\n",
    "    qd = self.exp.Q[idx].flatten()\n",
    "    dd = np.asarray(dd[idx].flatten(), dtype=np.float)                      # 2D flattened to 1D other wise dd*dd might become negative\n",
    "\n",
    "    if adjust_edges:\n",
    "        # willing to throw out some data, but the edges strictly correspond to qgrid\n",
    "        dq  = qgrid[1:]-qgrid[:-1]                                          # difference between two consequtive q values\n",
    "        dq1 = np.hstack(([dq[0]], dq))                                      # len(dq1) == len(qgrid)\n",
    "\n",
    "        bins = [qgrid[0]-dq1[0]/2]                                          # one value/ lower limit value qgrid[0]=0.005, dq1[0] = 0.001; bins = qgrid[0]-dq1[0]/2 = 0.0045 \n",
    "        bidx = []\n",
    "        binw = []\n",
    "\n",
    "        # bins will be len(qgrid) + 1\n",
    "        # bidx, binw will be len(qgrid)\n",
    "        for i in range(len(qgrid)):\n",
    "            el = qgrid[i] - dq1[i]/2                                        # qgrid[0]=0.005; el = qgrid[0]-dq1[0]/2 = 0.0045\n",
    "            eu = qgrid[i] + dq1[i]/2                                        # qgrid[0]=0.005; eu = qgrid[0]+dq1[0]/2 = 0.0055\n",
    "            if i==0 or np.fabs(el-bins[-1])<dq1[i]/100:                     # i=0 first element or \n",
    "                bins += [eu]                                                # first bin bins = [0.0045, 0.0055]\n",
    "                bidx += [True]                                              # \n",
    "                binw += [dq1[i]]                                            # \n",
    "            else:\n",
    "                bins += [el, eu]                                            # \n",
    "                bidx += [False, True]                                       # \n",
    "                binw += [dq1[i-1], dq1[i]]                                  # \n",
    "    else:\n",
    "        # keep all the data, but the histogrammed data will be less accurate \n",
    "        bins =  np.append([2*qgrid[0]-qgrid[1]], qgrid) \n",
    "        bins += np.append(qgrid , [2*qgrid[-1]-qgrid[-2]])\n",
    "        bins *= 0.5\n",
    "        bidx = np.ones(len(qgrid), dtype=bool)\n",
    "\n",
    "    norm = np.histogram(qd, bins=bins, weights=np.ones(len(qd)))[0][bidx] \n",
    "    qq   = np.histogram(qd, bins=bins, weights=qd)[0][bidx]\n",
    "    Iq   = np.histogram(qd, bins=bins, weights=dd)[0][bidx]\n",
    "    Iq2  = np.histogram(qd, bins=bins, weights=dd*dd)[0][bidx]\n",
    "\n",
    "    idx1 = (norm>min_norm_scale*np.arange(len(norm))**2)\n",
    "    qq[idx1] /= norm[idx1]\n",
    "    Iq[idx1] /= norm[idx1]\n",
    "    Iq2[idx1] /= norm[idx1]\n",
    "    dI = np.sqrt(Iq2-Iq*Iq)\n",
    "    dI[idx1] /= np.sqrt(norm[idx1])\n",
    "    qq[~idx1] = np.nan\n",
    "    Iq[~idx1] = np.nan\n",
    "    dI[~idx1] = np.nan\n",
    "\n",
    "    if interpolate:\n",
    "        Iq = np.interp(qgrid, qq, Iq)\n",
    "\n",
    "    return Iq,dI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixWithCoords:\n",
    "    # 2D data with coordinates\n",
    "    d = None\n",
    "    xc = None\n",
    "    yc = None \n",
    "    datatype = None\n",
    "    \n",
    "class DataType(Enum):\n",
    "    det = 1\n",
    "    qrqz = 2\n",
    "    qphi = 3\n",
    "    q = 4\n",
    "    xyq = 5      # Cartesian version of qphi\n",
    "\n",
    "class Data2d:\n",
    "    \"\"\" 2D scattering data class\n",
    "        stores the scattering pattern itself, \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img, timestamp=None, uid='', exp=None, label=''):\n",
    "        \"\"\" read 2D scattering pattern\n",
    "            img can be either a filename (rely on Fabio to deal with the file format) or a numpy array \n",
    "        \"\"\"\n",
    "        self.exp = None\n",
    "        self.timestamp = None\n",
    "        self.uid = None\n",
    "        self.data = MatrixWithCoords()\n",
    "        self.qrqz_data = MatrixWithCoords()\n",
    "        self.qphi_data = MatrixWithCoords()\n",
    "        self.label = label\n",
    "        \n",
    "        if isinstance(img, np.ndarray): \n",
    "            self.im = img\n",
    "            self.timestamp = timestamp\n",
    "            self.uid = uid\n",
    "        else:\n",
    "            raise Exception('Not sure how to create Data2d from img ...')\n",
    "\n",
    "        # self.im always stores the original image\n",
    "        # self.data store the array data after the flip operation\n",
    "        if exp is not None:\n",
    "            self.set_exp_para(exp)\n",
    "        else:\n",
    "            # temporarily set data to img, without a defined exp_para\n",
    "            self.flip(0)\n",
    "            self.height, self.width = self.data.d.shape\n",
    "            \n",
    "    def set_exp_para(self, exp):\n",
    "        self.flip(exp.flip)\n",
    "        (self.height, self.width) = np.shape(self.data.d)\n",
    "        self.exp = exp\n",
    "        if exp.ImageHeight!=self.height or exp.ImageWidth!=self.width:\n",
    "            raise Exception('mismatched shape between the data (%d,%d) and ExpPara (%d,%d).' % \n",
    "                            (self.width, self.height, exp.ImageWidth, exp.ImageHeight)) \n",
    "        self.data.xc = np.arange(self.width)\n",
    "        self.data.yc = np.flipud(np.arange(self.height)) \n",
    "        self.data.datatype = DataType.det\n",
    "        \n",
    "    def flip(self, flip):\n",
    "        \"\"\" this is a little complicated\n",
    "            if flip<0, do a mirror operation first\n",
    "            the absolute value of flip is the number of 90-deg rotations\n",
    "        \"\"\"  \n",
    "        self.data.d = np.asarray(self.im).copy()\n",
    "        if flip == 0:\n",
    "            return\n",
    "        if flip<0:\n",
    "            self.data.d = np.fliplr(self.data.d)\n",
    "            flip = -flip\n",
    "        for _ in range(flip):\n",
    "            self.data.d = np.rot90(self.data.d)\n",
    "\n",
    "    \n",
    "    def conv_Iq(self, qgrid, mask=None, cor_factor=1, \n",
    "                adjust_edges=True, interpolate=True, min_norm_scale=0.002):\n",
    "        \n",
    "        dd = self.data.d/cor_factor                                             # divide all data by correction factor\n",
    "        \n",
    "        # Pilatus might use negative values to mark dead pixels\n",
    "        idx = (self.data.d>=0)                                                  # idx of values >=0\n",
    "        if mask is not None:\n",
    "            idx &= ~(mask.map)\n",
    "\n",
    "        qd = self.exp.Q[idx].flatten()\n",
    "        dd = np.asarray(dd[idx].flatten(), dtype=np.float)                      # 2D flattened to 1D other wise dd*dd might become negative\n",
    "\n",
    "        if adjust_edges:\n",
    "            # willing to throw out some data, but the edges strictly correspond to qgrid\n",
    "            dq  = qgrid[1:]-qgrid[:-1]                                          # difference between two consequtive q values\n",
    "            dq1 = np.hstack(([dq[0]], dq))                                      # len(dq1) == len(qgrid)\n",
    "\n",
    "            bins = [qgrid[0]-dq1[0]/2]                                          # one value/ lower limit value qgrid[0]=0.005, dq1[0] = 0.001; bins = qgrid[0]-dq1[0]/2 = 0.0045 \n",
    "            bidx = []\n",
    "            binw = []\n",
    "            \n",
    "            # bins will be len(qgrid) + 1\n",
    "            # bidx, binw will be len(qgrid)\n",
    "            for i in range(len(qgrid)):\n",
    "                el = qgrid[i] - dq1[i]/2                                        # qgrid[0]=0.005; el = qgrid[0]-dq1[0]/2 = 0.0045\n",
    "                eu = qgrid[i] + dq1[i]/2                                        # qgrid[0]=0.005; eu = qgrid[0]+dq1[0]/2 = 0.0055\n",
    "                if i==0 or np.fabs(el-bins[-1])<dq1[i]/100:                     # i=0 first element or \n",
    "                    bins += [eu]                                                # first bin bins = [0.0045, 0.0055]\n",
    "                    bidx += [True]                                              # \n",
    "                    binw += [dq1[i]]                                            # \n",
    "                else:\n",
    "                    bins += [el, eu]                                            # \n",
    "                    bidx += [False, True]                                       # \n",
    "                    binw += [dq1[i-1], dq1[i]]                                  # \n",
    "        else:\n",
    "            # keep all the data, but the histogrammed data will be less accurate \n",
    "            bins =  np.append([2*qgrid[0]-qgrid[1]], qgrid) \n",
    "            bins += np.append(qgrid , [2*qgrid[-1]-qgrid[-2]])\n",
    "            bins *= 0.5\n",
    "            bidx = np.ones(len(qgrid), dtype=bool)\n",
    "\n",
    "        norm = np.histogram(qd, bins=bins, weights=np.ones(len(qd)))[0][bidx] \n",
    "        qq   = np.histogram(qd, bins=bins, weights=qd)[0][bidx]\n",
    "        Iq   = np.histogram(qd, bins=bins, weights=dd)[0][bidx]\n",
    "        Iq2  = np.histogram(qd, bins=bins, weights=dd*dd)[0][bidx]\n",
    "\n",
    "        idx1 = (norm>min_norm_scale*np.arange(len(norm))**2)\n",
    "        qq[idx1] /= norm[idx1]\n",
    "        Iq[idx1] /= norm[idx1]\n",
    "        Iq2[idx1] /= norm[idx1]\n",
    "        dI = np.sqrt(Iq2-Iq*Iq)\n",
    "        dI[idx1] /= np.sqrt(norm[idx1])\n",
    "        qq[~idx1] = np.nan\n",
    "        Iq[~idx1] = np.nan\n",
    "        dI[~idx1] = np.nan\n",
    "\n",
    "        if interpolate:\n",
    "            Iq = np.interp(qgrid, qq, Iq)\n",
    "\n",
    "        return Iq,dI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size_list = ['xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large']\n",
    "\n",
    "def get_font_size(size_index):\n",
    "    \"\"\" \"medium\" has size_index of 0\n",
    "        the size_index is negative for smaller fonts and possitive for larger ones  \n",
    "    \"\"\"\n",
    "    if size_index in font_size_list:\n",
    "        i = font_size_list.index(size_index)\n",
    "    else:\n",
    "        i = int(size_index)+3\n",
    "        if i<0:\n",
    "            i = 0\n",
    "        elif i>=len(font_size_list):\n",
    "            i = len(font_size_list)-1\n",
    "    return i-3,font_size_list[i]\n",
    "\n",
    "class Data1d:\n",
    "    def __init__(self, trandMode=None):\n",
    "        self.comments = \"\"\n",
    "        self.label = \"data\"\n",
    "        self.overlaps = []\n",
    "        self.raw_data = {}\n",
    "        self.timestamp = None\n",
    "        self.trans = 0\n",
    "        \n",
    "    def scale(self, sc):\n",
    "        \"\"\"\n",
    "        scale the data by factor sc\n",
    "        \"\"\"\n",
    "        if sc <= 0:\n",
    "            print(\"scaling factor is non-positive: %f\" % sc)\n",
    "        self.data *= sc\n",
    "        self.err *= sc\n",
    "        self.trans *= sc\n",
    "        self.comments += \"# data is scaled by %f.\\n\" % sc\n",
    "        if len(self.overlaps) != 0:\n",
    "            for ov in self.overlaps:\n",
    "                ov['raw_data1'] *= sc\n",
    "                ov['raw_data2'] *= sc\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def load_from_2D(self, image, exp_para, qgrid, pre_process=None, \n",
    "                     mask=None, save_ave=False, debug=False, label=None):\n",
    "        \"\"\"\n",
    "        image: a filename, or a Data2d instance, or a numpy array\n",
    "        qgrid: for the 1D data\n",
    "        exp_para: ExpPara\n",
    "        mask: no longer used, extract from exp_para\n",
    "        \"\"\"\n",
    "        self.qgrid = qgrid\n",
    "        mask = exp_para.mask\n",
    "\n",
    "        if debug==True:\n",
    "            print(\"loading data from 2D image: \", label)\n",
    "    \n",
    "        if isinstance(image, Data2d):\n",
    "            d2 = image\n",
    "        else:\n",
    "            d2 = Data2d(image, exp=exp_para)\n",
    "            self.timestamp = d2.timestamp\n",
    "            self.label = d2.label\n",
    "            self.timestamp = d2.timestamp\n",
    "            \n",
    "        if label is not None:\n",
    "            self.label = label\n",
    "            \n",
    "        # deal with things like dark current, flat field, and dezinger corrections on the 2D data\n",
    "        if pre_process is not None:\n",
    "            pre_process(d2.data)\n",
    "        \n",
    "\n",
    "        self.data,self.err = d2.conv_Iq(qgrid, mask,\n",
    "                                        cor_factor = exp_para.FSA*exp_para.FPol)\n",
    "                                        #cor_factor = exp_para.FPol)  \n",
    "        if isinstance(image, np.ndarray):\n",
    "            del d2      # d2 is only used temporarily\n",
    "        \n",
    "        if save_ave and isinstance(image, str):\n",
    "            self.save(image + \".ave\", debug=debug)     \n",
    "        \n",
    "    def plot(self, ax=None, scale=1., fontsize='large'):\n",
    "        i_fs = get_font_size(fontsize)[0]\n",
    "        if ax is None:\n",
    "            plt.figure()\n",
    "            plt.subplots_adjust(bottom=0.15)\n",
    "            ax = plt.gca()\n",
    "        ax.set_xlabel(\"$q (\\AA^{-1})$\", fontsize=get_font_size(i_fs)[1])\n",
    "        ax.set_ylabel(\"$I$\", fontsize=get_font_size(i_fs)[1])\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.errorbar(self.qgrid, self.data*scale, self.err*scale, label=self.label)\n",
    "        for ov in self.overlaps:\n",
    "            ax.plot(ov['q_overlap'], ov['raw_data1']*scale, \"v\")\n",
    "            ax.plot(ov['q_overlap'], ov['raw_data2']*scale, \"^\")\n",
    "        leg = ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "        for t in leg.get_texts():\n",
    "            t.set_fontsize(get_font_size(i_fs-2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_names = [{\"_SAXS\": \"pil1M_image\",\n",
    "              \"_WAXS1\": \"pilW1_image\",\n",
    "              \"_WAXS2\": \"pilW2_image\"}, \n",
    "             {\"_SAXS\": \"pil1M_ext_image\",\n",
    "              \"_WAXS1\": \"pilW1_ext_image\",\n",
    "              \"_WAXS2\": \"pilW2_ext_image\"}]\n",
    "\n",
    "class trans_mode(Enum):\n",
    "    external = 0\n",
    "    from_waxs = 2\n",
    "\n",
    "def lsh5(hd, prefix='', top_only=False, silent=False):\n",
    "    \"\"\" list the content of a HDF5 file\n",
    "        \n",
    "        hd: a handle returned by h5py.File()\n",
    "        prefix: use to format the output when lsh5() is called recursively\n",
    "        top_only: returns the names of the top-level groups\n",
    "        silent: suppress printouts if True\n",
    "    \"\"\"\n",
    "    if top_only:\n",
    "        tp_grps = list(hd.keys())\n",
    "        if not silent:\n",
    "            print(tp_grps)\n",
    "        return tp_grps\n",
    "    for k in list(hd.keys()):\n",
    "        print(prefix, k)\n",
    "        if isinstance(hd[k], h5py.Group):\n",
    "            print(list(hd[k].attrs.items()))\n",
    "            lsh5(hd[k], prefix+\"=\")\n",
    "\n",
    "def strip_name(s):\n",
    "    strs = [\"_SAXS\",\"_WAXS1\",\"_WAXS2\",\".cbf\",\".tif\"]\n",
    "    for ts in strs:\n",
    "        if ts in s:\n",
    "            ss = s.split(ts)\n",
    "            s = \"\".join(ss)\n",
    "    return s\n",
    "            \n",
    "def common_name(s1, s2):\n",
    "    s1 = strip_name(s1)\n",
    "    s2 = strip_name(s2)\n",
    "    l = len(s1)\n",
    "    if len(s2) < l:\n",
    "        l = len(s2)\n",
    "\n",
    "    s = \"\"\n",
    "    for i in range(l):\n",
    "        if s1[i] == s2[i]:\n",
    "            s += s1[i]\n",
    "        else:\n",
    "            break\n",
    "    if len(s) < 1:\n",
    "        s = s1.copy()\n",
    "    return s.rstrip(\"-_ \")\n",
    "            \n",
    "def merge_d1s(d1s, detectors, save_merged=False, debug=False):\n",
    "    \"\"\" utility function to merge 1D data sets, using functions under slnxs \n",
    "        d1s should contain data corresponding to detectors\n",
    "    \"\"\"\n",
    "    s0 = Data1d()\n",
    "    s0.qgrid = d1s[0].qgrid\n",
    "    d_tot = np.zeros(s0.qgrid.shape)\n",
    "    d_max = np.zeros(s0.qgrid.shape)\n",
    "    d_min = np.zeros(s0.qgrid.shape)+1.e32\n",
    "    e_tot = np.zeros(s0.qgrid.shape)\n",
    "    c_tot = np.zeros(s0.qgrid.shape)\n",
    "    label = None\n",
    "    comments = \"\"\n",
    "                \n",
    "    for d1 in d1s:        \n",
    "        # empty part of the data is nan\n",
    "        idx = ~np.isnan(d1.data)\n",
    "        d_tot[idx] += d1.data[idx]\n",
    "        e_tot[idx] += d1.err[idx]\n",
    "        c_tot[idx] += 1\n",
    "\n",
    "        idx1 = (np.ma.fix_invalid(d1.data, fill_value=-1)>d_max).data\n",
    "        d_max[idx1] = d1.data[idx1]\n",
    "        idx2 = (np.ma.fix_invalid(d1.data, fill_value=1e32)<d_min).data\n",
    "        d_min[idx2] = d1.data[idx2]\n",
    "            \n",
    "        comments += d1.comments\n",
    "        if label is None:\n",
    "            label = d1.label\n",
    "        else:\n",
    "            label = common_name(label, d1.label)\n",
    "        \n",
    "    s0.data = d_tot\n",
    "    s0.err = e_tot\n",
    "    idx = (c_tot>1)\n",
    "    s0.overlaps.append({'q_overlap': s0.qgrid[idx],\n",
    "                        'raw_data1': d_max[idx],\n",
    "                        'raw_data2': d_min[idx]})\n",
    "    s0.data[idx] /= c_tot[idx]\n",
    "    s0.err[idx] /= np.sqrt(c_tot[idx])\n",
    "    s0.label = label\n",
    "    s0.comments = comments # .replace(\"# \", \"## \")\n",
    "    if save_merged:\n",
    "        s0.save(s0.label+\".dd\", debug=debug)\n",
    "        \n",
    "    return s0\n",
    "            \n",
    "            \n",
    "def proc_d1merge(args):\n",
    "    \"\"\" utility function to perfrom azimuthal average and merge detectors\n",
    "    \"\"\"\n",
    "    images,sn,nframes,starting_frame_no,debug,detectors,qgrid,reft,save_1d,save_merged = args\n",
    "    ret = {'merged': []}\n",
    "    sc = {}\n",
    "    \n",
    "    for det in detectors:\n",
    "        ret[det.extension] = []\n",
    "        if det.fix_scale is not None:\n",
    "            sc[det.extension] = 1./det.fix_scale\n",
    "\n",
    "    if debug is True:\n",
    "        print(\"processing started: sample = %s, starting frame = #%d\" % (sn, starting_frame_no))\n",
    "    for i in range(nframes):\n",
    "        for det in detectors:\n",
    "            dt = Data1d()\n",
    "            label = \"%s_f%05d%s\" % (sn, i+starting_frame_no, det.extension)\n",
    "            dt.load_from_2D(images[det.extension][i], \n",
    "                            det.exp_para, qgrid, det.pre_process, det.exp_para.mask,\n",
    "                            save_ave=False, debug=debug, label=label)\n",
    "            dt.scale(sc[det.extension])\n",
    "            ret[det.extension].append(dt)\n",
    "    \n",
    "        dm = merge_d1s([ret[det.extension][i] for det in detectors], detectors, save_merged, debug)\n",
    "        ret['merged'].append(dm)\n",
    "            \n",
    "    if debug is True:\n",
    "        print(\"processing completed: \", sn, starting_frame_no)\n",
    "\n",
    "    return [sn, starting_frame_no, ret]            \n",
    "\n",
    "def pack_d1(data, ret_trans=True):\n",
    "    \"\"\" utility function to creat a list of [intensity, error] from a Data1d object \n",
    "        or from a list of Data1s objects\n",
    "    \"\"\"\n",
    "    if isinstance(data, Data1d):\n",
    "        if ret_trans:\n",
    "            return np.asarray([data.data,data.err]), data.trans\n",
    "        else:\n",
    "            return np.asarray([data.data,data.err])\n",
    "    elif isinstance(data, list):\n",
    "        tvs = [d.trans for d in data]\n",
    "        return np.asarray([pack_d1(d, False) for d in data]),tvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h5xs():\n",
    "    \"\"\" Scattering data in transmission geometry\n",
    "        Transmitted beam intensity can be set either from the water peak (sol), or from intensity monitor.\n",
    "        Data processing can be done either in series, or in parallel. Serial processing can be forced.\n",
    "        \n",
    "    \"\"\"    \n",
    "    def __init__(self, fn, exp_setup=None, transField='', save_d1=True):\n",
    "        \"\"\" exp_setup: [detectors, qgrid]\n",
    "            transField: the intensity monitor field packed by suitcase from databroker\n",
    "            save_d1: save newly processed 1d data back to the h5 file\n",
    "        \"\"\"\n",
    "        self.d1s = {}\n",
    "        self.detectors = None\n",
    "        self.samples = []\n",
    "        self.attrs = {}\n",
    "        # name of the dataset that contains transmitted beam intensity, e.g. em2_current1_mean_value\n",
    "        self.transField = None  \n",
    "\n",
    "        self.fn = fn\n",
    "        self.save_d1 = save_d1\n",
    "        self.fh5 = h5py.File(self.fn, \"r+\")   # file must exist\n",
    "        if exp_setup==None:     # assume the h5 file will provide the detector config\n",
    "            self.qgrid = self.read_detectors()\n",
    "        else:\n",
    "            self.detectors, self.qgrid = exp_setup\n",
    "            self.save_detectors()\n",
    "        self.list_samples(quiet=True)\n",
    "        # find out what are the fields corresponding to the 2D detectors\n",
    "        # at LiX there are two possibilities\n",
    "        data_fields = list(self.fh5[self.samples[0]+'/primary/data'])\n",
    "        self.det_name = None\n",
    "        # these are the detectors that are present in the data\n",
    "        d_dn = [d.extension for d in self.detectors]\n",
    "        for det_name in det_names:\n",
    "            for k in set(det_name.keys()).difference(d_dn):\n",
    "                del det_name[k]\n",
    "            if set(det_name.values()).issubset(data_fields):\n",
    "                self.det_name = det_name\n",
    "                break\n",
    "        if self.det_name is None:\n",
    "            print('fields in the h5 file: ', data_fields)\n",
    "            raise Exception(\"Could not find the data corresponding to the detectors.\")\n",
    "        if transField=='':\n",
    "            # \"2,\" --> self.transField = '' and self.transMode = trans_mode.from_waxs\n",
    "            if 'trans' in self.fh5.attrs:\n",
    "                [v, self.transField] = self.fh5.attrs['trans'].split(',')\n",
    "                self.transMode = trans_mode(int(v))\n",
    "                return\n",
    "            else:\n",
    "                self.transMode = trans_mode.from_waxs\n",
    "                self.transField = ''\n",
    "        elif transField not in data_fields:\n",
    "            print(\"invalid filed for transmitted intensity: \", transField)\n",
    "            raise Exception()\n",
    "        else:\n",
    "            self.transField = transField\n",
    "            self.transMode = trans_mode.external\n",
    "        self.fh5.attrs['trans'] = ','.join([str(self.transMode.value), self.transField])  # \"0,em2_sum_all_mean_value\"\n",
    "        self.fh5.flush()\n",
    "            \n",
    "    def save_detectors(self):\n",
    "        dets_attr = [det.pack_dict() for det in self.detectors]\n",
    "        self.fh5.attrs['detectors'] = json.dumps(dets_attr)\n",
    "        self.fh5.attrs['qgrid'] = list(self.qgrid)\n",
    "        self.fh5.flush()\n",
    "    \n",
    "    def read_detectors(self):\n",
    "        dets_attr = self.fh5.attrs['detectors']\n",
    "        qgrid = self.fh5.attrs['qgrid']\n",
    "        self.detectors = [create_det_from_attrs(attrs) for attrs in json.loads(dets_attr)]  \n",
    "        return np.asarray(qgrid)\n",
    "    \n",
    "    def list_samples(self, quiet=False):\n",
    "        self.samples = lsh5(self.fh5, top_only=True, silent=True)\n",
    "        if not quiet:\n",
    "            print(self.samples)\n",
    "    \n",
    "    def save_d1s(self, sn=None, debug=False):\n",
    "        \"\"\"\n",
    "        save the 1d data in memory to the hdf5 file \n",
    "        processed data go under the group sample_name/processed\n",
    "        assume that the shape of the data is unchanged\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.save_d1 is False:\n",
    "            print(\"requested to save_d1s() but h5xs.save_d1 is False.\")\n",
    "            return\n",
    "        if sn==None:\n",
    "            self.list_samples(quiet=True)\n",
    "            for sn in self.samples:\n",
    "                self.save_d1s(sn)\n",
    "        \n",
    "        fh5 = self.fh5        \n",
    "        if \"processed\" not in list(lsh5(fh5[sn], top_only=True, silent=True)):\n",
    "            grp = fh5[sn].create_group(\"processed\")\n",
    "        else:\n",
    "            grp = fh5[sn+'/processed']\n",
    "            g0 = lsh5(grp, top_only=True, silent=True)[0]\n",
    "            if grp[g0][0].shape[1]!=len(self.qgrid): # if grp[g0].value[0].shape[1]!=len(self.qgrid):\n",
    "                # new size for the data\n",
    "                del fh5[sn+'/processed']\n",
    "                grp = fh5[sn].create_group(\"processed\")\n",
    "        \n",
    "        # these attributes are not necessarily available when save_d1s() is called\n",
    "        if sn in list(self.attrs.keys()):\n",
    "            for k in list(self.attrs[sn].keys()):\n",
    "                grp.attrs[k] = self.attrs[sn][k]\n",
    "                if debug is True:\n",
    "                    print(\"writting attribute to %s: %s\" % (sn, k))\n",
    "\n",
    "        ds_names = lsh5(grp, top_only=True, silent=True)\n",
    "        for k in list(self.d1s[sn].keys()):\n",
    "            data,tvs = pack_d1(self.d1s[sn][k])\n",
    "            if debug is True:\n",
    "                print(\"writting attribute to %s: %s\" % (sn, k))\n",
    "            if k not in ds_names:\n",
    "                grp.create_dataset(k, data=data)\n",
    "            else:\n",
    "                grp[k][...] = data   \n",
    "\n",
    "            # save trans values for processed data\n",
    "            # before 1d data merge, the trans value should be 0              \n",
    "            # on the other hand there could be data collected with the beam off, therefore trans=0\n",
    "            if (np.asarray(tvs)>0).any(): \n",
    "                grp[k].attrs['trans'] = tvs\n",
    "                \n",
    "        fh5.flush()\n",
    "            \n",
    "    def load_data(self, update_only=False, detectors=None,\n",
    "           reft=-1, save_1d=False, save_merged=False, debug=False, N=8, max_c_size=0):\n",
    "        \"\"\" assume multiple samples, parallel-process by sample\n",
    "            use Pool to limit the number of processes; \n",
    "            access h5 group directly in the worker process\n",
    "        \"\"\"\n",
    "        if debug is True:\n",
    "            print(\"start processing: load_data()\")\n",
    "            t1 = time.time()\n",
    "        \n",
    "        fh5 = self.fh5\n",
    "        self.samples = lsh5(fh5, top_only=True, silent=(not debug))\n",
    "        \n",
    "        results = {}\n",
    "        pool = mp.Pool(N)\n",
    "        jobs = []\n",
    "        \n",
    "        for sn in self.samples:\n",
    "            if sn not in list(self.attrs.keys()):\n",
    "                self.attrs[sn] = {}\n",
    "            if 'buffer' in list(fh5[sn].attrs):\n",
    "                self.buffer_list[sn] = fh5[sn].attrs['buffer'].split('  ')\n",
    "            if update_only and sn in list(self.d1s.keys()):\n",
    "                self.load_d1s(sn)   # load processed data saved in the file\n",
    "                continue\n",
    "                                    \n",
    "            self.d1s[sn] = {}\n",
    "            results[sn] = {}\n",
    "            dset = fh5[\"%s/primary/data\" % sn]\n",
    "            \n",
    "            s = dset[\"%s\" % self.det_name[self.detectors[0].extension]].shape\n",
    "            if len(s)==3 or len(s)==4:\n",
    "                self.n_total_frames = s[0]\n",
    "            else:\n",
    "                raise Exception(\"don't know how to handle shape:\", )\n",
    "            if self.n_total_frames<N*N/2:\n",
    "                Np = 1\n",
    "                c_size = N\n",
    "            else:\n",
    "                Np = N\n",
    "                c_size = int(self.n_total_frames/N)\n",
    "                if max_c_size>0 and c_size>max_c_size:\n",
    "                    Np = int(self.n_total_frames/max_c_size)+1\n",
    "                    c_size = int(self.n_total_frames/Np)\n",
    "                    \n",
    "            # process data in group in hope to limit memory use\n",
    "            # the raw data could be stored in a 1d or 2d array\n",
    "            if detectors is None:\n",
    "                detectors = self.detectors\n",
    "            for i in range(Np):\n",
    "                if i==Np-1:\n",
    "                    nframes = self.n_total_frames - c_size*(Np-1)   # 3721 - 465*(7) = 466\n",
    "                else:\n",
    "                    nframes = c_size    # 465\n",
    "                    \n",
    "                if len(s)==3:\n",
    "                    images = {}\n",
    "                    for det in detectors:\n",
    "                        gn = f'{self.det_name[det.extension]}'\n",
    "                        images[det.extension] = dset[gn][i*c_size:i*c_size+nframes]    \n",
    "\n",
    "                    if N>1: # multi-processing, need to keep track of total number of active processes                    \n",
    "                        job = pool.map_async(proc_d1merge, [(images, sn, nframes, i*c_size, debug,\n",
    "                                                             detectors, self.qgrid, reft, save_1d, save_merged)])\n",
    "                        jobs.append(job)\n",
    "                    else: # serial processing\n",
    "                        [sn, fr1, data] = proc_d1merge((images, sn, nframes, i*c_size, debug, \n",
    "                                                        detectors, self.qgrid, reft, save_1d, save_merged)) \n",
    "                        results[sn][fr1] = data                \n",
    "                else: # len(s)==4\n",
    "                    for j in range(s[1]):\n",
    "                        images = {}\n",
    "                        for det in detectors:\n",
    "                            gn = f'{self.det_name[det.extension]}'\n",
    "                            images[det.extension] = dset[gn][i*c_size:i*c_size+nframes, j]\n",
    "                        if N>1: # multi-processing, need to keep track of total number of active processes\n",
    "                            job = pool.map_async(proc_d1merge, [(images, sn, nframes, i*c_size+j*s[0], debug,\n",
    "                                                                 detectors, self.qgrid, reft, save_1d, save_merged)])\n",
    "                            jobs.append(job)\n",
    "                        else: # serial processing\n",
    "                            [sn, fr1, data] = proc_d1merge((images, sn, nframes, i*c_size+j*s[0], debug, \n",
    "                                                            detectors, self.qgrid, reft, save_1d, save_merged)) \n",
    "                            results[sn][fr1] = data                \n",
    "\n",
    "        if N>1:\n",
    "            for job in jobs:\n",
    "                [sn, fr1, data] = job.get()[0]\n",
    "                results[sn][fr1] = data           # [sn, starting_frame_no, ret]\n",
    "                print(\"data received: sn=%s, fr1=%d\" % (sn,fr1) )\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        for sn in self.samples:\n",
    "            if sn not in results.keys():\n",
    "                continue\n",
    "            data = {}\n",
    "            frns = list(results[sn].keys())\n",
    "            frns.sort()\n",
    "            for k in results[sn][frns[0]].keys():\n",
    "                data[k] = []  # ret = {'merged': []}\n",
    "                for frn in frns:                             # dm = merge_d1s([ret[det.extension][i] for det in detectors], detectors, save_merged, debug)\n",
    "                    data[k].extend(results[sn][frn][k])      # ret['merged'].append(dm)\n",
    "            self.d1s[sn] = data           \n",
    "        \n",
    "        self.save_d1s(debug=debug)\n",
    "        if debug is True:\n",
    "            t2 = time.time()\n",
    "            print(\"done, time lapsed: %.2f sec\" % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9d3ba9d53911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdt\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mh5xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mica.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqgrid2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5ea4aa302540>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, exp_setup, transField, save_d1)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# \"2,\" --> self.transField = '' and self.transMode = trans_mode.from_waxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'trans'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransField\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransMode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dt  = h5xs(\"mica.h5\", [de.detectors, qgrid2])\n",
    "    dt.load_data(N=4, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.n_total_frames)\n",
    "n = len(np.asarray(dt.d1s['2048_B8']['_WAXS2'][0].data))\n",
    "diff_patterns = np.zeros((1,n))\n",
    "for i in range(dt.n_total_frames):\n",
    "    diff_patterns = np.vstack([diff_patterns, np.asarray(dt.d1s['1934_B8']['merged'][i].data).reshape(1,n)])\n",
    "\n",
    "diff_patterns = np.delete(diff_patterns,obj=0,axis=0)\n",
    "diff_patterns = np.expand_dims(diff_patterns,axis=1)\n",
    "diff_patterns = np.transpose(diff_patterns,axes=[1,2,0])        # reshape (1, 690, 3721)\n",
    "print(diff_patterns.shape)\n",
    "\n",
    "BandAngles = qgrid2\n",
    "print(f'BandAngles shape = {BandAngles.shape}')                 # shape (690,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diff_patterns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ffdac066a8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_intensity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_patterns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mWidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mHeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diff_patterns' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from matplotlib import pyplot  as plt\n",
    "import seaborn as sns\n",
    "\n",
    "[_,n_intensity,n_patterns] = np.shape(diff_patterns);   \n",
    "Width = int(np.sqrt(n_patterns));\n",
    "Height = int(np.sqrt(n_patterns));    \n",
    "print('Intensity ' + str(n_intensity) + ' Diff. Patterns ' + str(Width))\n",
    "\n",
    "\n",
    "# -*- Find Maximum Value -*- \n",
    "_, QMAX, SMAX = np.where(diff_patterns == np.max(diff_patterns))    # Maximum Q and Maximum Diff. Patt.\n",
    "\n",
    "# -*- Plot Intensity -*- \n",
    "PlotDiffRange = np.linspace(SMAX,SMAX + 19, 20,dtype=np.int32);    # 20 Plots from Highest Intensity \n",
    "l=0;\n",
    "\n",
    "plt.figure;\n",
    "fig,axes = plt.subplots(5,4, figsize=(15,8), sharex=True, sharey=True);\n",
    "R,C = np.shape(axes)\n",
    "for r in range(R):\n",
    "    for c in range(C):\n",
    "        axes[r][c].plot(BandAngles, np.log(np.ravel(diff_patterns[:,:,PlotDiffRange[l]])))\n",
    "        axes[r][c].set_title('Sample = ' + str(PlotDiffRange[l] + 1), fontsize=8)\n",
    "        l = l + 1;\n",
    "plt.setp(axes[-1,:], xlabel = 'Q')\n",
    "plt.setp(axes[:,0],  ylabel = 'log Intensity')\n",
    "fig.show()\n",
    "\n",
    "# -*- Plot Heat Map -*- \n",
    "plt.figure(figsize=(16,9))\n",
    "circ_value = np.zeros([1,1,n_patterns]);             # circ_avg. value\n",
    "for l in range(n_intensity) :\n",
    "    circ_value = circ_value + diff_patterns[:,l,:];\n",
    "circ_value = circ_value/n_intensity;\n",
    "img_orig = np.reshape(circ_value,[Height,Width]);\n",
    "ll = sns.heatmap(img_orig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAXS [0.007 , 0.3700000000000002] WAXS [0.29500000000000015 , 2.6400000000000023] \n"
     ]
    }
   ],
   "source": [
    "print('SAXS [{} , {}] WAXS [{} , {}] '.format(qgrid2[2], qgrid2[124], qgrid2[109], qgrid2[578]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAXS Fix scale 1.0. \n",
      "WAXS Fix scale 0.010404022787155612\n"
     ]
    }
   ],
   "source": [
    "print('SAXS Fix scale {}. \\nWAXS Fix scale {}'.format(1/de.detectors[0].fix_scale, \\\n",
    "                                                    1/de.detectors[1].fix_scale))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
